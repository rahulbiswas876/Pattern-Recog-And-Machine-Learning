{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions to students:\n",
    "\n",
    "1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.\n",
    "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
    "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
    "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
    "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
    "    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)\n",
    "    \n",
    "2. You are not allowed to insert new cells in the submitted notebook.\n",
    "\n",
    "3. You are not allowed to import any extra packages.\n",
    "\n",
    "4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.\n",
    "\n",
    "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. \n",
    "\n",
    "6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.\n",
    "\n",
    "7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with \"run all\" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.\n",
    "\n",
    "8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks. \n",
    "\n",
    "9. You may discuss broad ideas with friends, but all code must be written by yourself.\n",
    "\n",
    "9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :\"X_train, Y_train, X_test, Y_test\". In that order. The meaning of the 4 arrays can be easily inferred from their names.\n",
    "\n",
    "10. All plots must be labelled properly, all tables must have rows and columns named properly.\n",
    "\n",
    "11. Plotting the data and prediction is highly encouraged for debugging. But remove debugging/understanding code before submitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeRead\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1: Learning Binary Bayes Classifiers from data with Max. Likelihood \n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. \n",
    "\n",
    "1a) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, I)$ and  $X|Y=1 \\sim \\mathcal{N}(\\mu_+, I)$. *(Same known covariance)*\n",
    "\n",
    "1b) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma)$ *(Same unknown covariance)*\n",
    "\n",
    "1c) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma_-)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma_+)$ *(different unknown covariance)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell type : CodeWrite\n",
    "\n",
    "def Bayes1a(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1a.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "#     print(X_train[0:50,:])\n",
    "#     print(Y_train[0:50])\n",
    "    X_neg = X_train[ Y_train == -1,:]\n",
    "    X_pos = X_train[ Y_train == +1,:]\n",
    "    \n",
    "    \n",
    "    mu_neg = np.mean(X_neg, axis=0)\n",
    "    mu_pos = np.mean(X_pos, axis=0)\n",
    "    \n",
    "#     print(X_train[0:2, :])\n",
    "#     print(mu_neg)\n",
    "#     print(mu_pos)\n",
    "    \n",
    "    prior_neg = X_neg.shape[0]/X_train.shape[0]\n",
    "    prior_pos = X_pos.shape[0]/X_train.shape[0]\n",
    "    \n",
    "#     print(prior_neg, prior_pos)\n",
    "    \n",
    "#     print(np.sum((X_test - mu_pos)**2,axis=1)[0:2])\n",
    "    \n",
    "    #spl 2 class 0-1 loss, discriminant func: g(x) = g_-ve(x) - g_+ve(x) >= 0 then classify x as -ve class\n",
    "    g_Xtest = (1./2.)*np.sum((X_test - mu_pos)**2,axis=1) - \\\n",
    "              (1./2.)*np.sum((X_test - mu_neg)**2,axis=1) + \\\n",
    "              np.log(prior_neg/ prior_pos)\n",
    "    \n",
    "    Y_test_pred = np.zeros(g_Xtest.shape[0])\n",
    "    Y_test_pred[g_Xtest > 0] =  -1.\n",
    "    Y_test_pred[g_Xtest <= 0] = 1. \n",
    "    \n",
    "    return Y_test_pred\n",
    "    \n",
    "    \n",
    "#     g_Xtrain = (1./2.)*np.sum((X_train - mu_pos)**2,axis=1) - \\\n",
    "#                (1./2.)*np.sum((X_train - mu_neg)**2,axis=1) + \\\n",
    "#                np.log(prior_neg/ prior_pos)\n",
    "    \n",
    "    \n",
    "#     Y_train_predicted = np.zeros(g_Xtrain.shape[0])\n",
    "#     Y_train_predicted[g_Xtrain > 0] =  -1.\n",
    "#     Y_train_predicted[g_Xtrain <= 0] = 1 \n",
    "    \n",
    "#     print(Y_train.shape[0])\n",
    "#     print( np.count_nonzero(Y_train == Y_train_predicted))\n",
    "    \n",
    "#     plt.scatter(X_train[Y_train == -1][:,0], X_train[Y_train == -1][:,1], alpha=0.2, color='r', marker='o')\n",
    "#     plt.scatter(X_train[Y_train == 1][:,0], X_train[Y_train == 1][:,1],  alpha=0.2, color='g', marker='o')\n",
    "    \n",
    "#     print('train accuracy:', np.count_nonzero(Y_train == Y_train_predicted)/Y_train.shape[0])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(X_train_neg)\n",
    "    \n",
    "    \n",
    "def Bayes1b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_neg = X_train[ Y_train == -1,:]\n",
    "    X_pos = X_train[ Y_train == +1,:]\n",
    "    \n",
    "    \n",
    "    mu_neg = np.mean(X_neg, axis=0)\n",
    "    mu_pos = np.mean(X_pos, axis=0)\n",
    "    \n",
    "    prior_neg = X_neg.shape[0]/X_train.shape[0]\n",
    "    prior_pos = X_pos.shape[0]/X_train.shape[0]\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    comm_cov = np.cov(X_train.T)\n",
    "    print(comm_cov.shape)\n",
    "    \n",
    "    inv_cov = np.linalg.inv(comm_cov)\n",
    "    \n",
    "    discri_neg = (-1/2)*np.dot(np.dot((X_test - mu_neg),inv_cov), (X_test - mu_neg).T)\n",
    "    discri_neg = np.diag(discri_neg)\n",
    "    discri_neg = discri_neg + np.log(prior_neg)\n",
    "    \n",
    "    discri_pos = (-1/2)*np.dot(np.dot((X_test - mu_pos),inv_cov), (X_test - mu_pos).T)\n",
    "    discri_pos = np.diag(discri_pos)\n",
    "    discri_pos = discri_pos + np.log(prior_pos)\n",
    "    \n",
    "    #classifiy to class -1 if discri_neg > discri_pos\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    Y_test_pred[discri_neg > discri_pos] = -1\n",
    "    Y_test_pred[discri_neg <= discri_pos] = +1\n",
    "    \n",
    "    return Y_test_pred\n",
    "    \n",
    "def Bayes1c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_neg = X_train[ Y_train == -1,:]\n",
    "    X_pos = X_train[ Y_train == +1,:]\n",
    "    \n",
    "    \n",
    "    mu_neg = np.mean(X_neg, axis=0)\n",
    "    mu_pos = np.mean(X_pos, axis=0)\n",
    "    \n",
    "    prior_neg = X_neg.shape[0]/X_train.shape[0]\n",
    "    prior_pos = X_pos.shape[0]/X_train.shape[0]\n",
    "\n",
    "    cov_neg = np.cov(X_neg.T)\n",
    "    cov_pos = np.cov(X_pos.T)\n",
    "    \n",
    "    inv_cov_neg = np.linalg.inv(cov_neg)\n",
    "    inv_cov_pos = np.linalg.inv(cov_pos)\n",
    "    \n",
    "    discri_neg = (-1/2)*np.dot(np.dot((X_test - mu_neg),inv_cov_neg), (X_test - mu_neg).T)\n",
    "    discri_neg = np.diag(discri_neg)\n",
    "    discri_neg = discri_neg + np.log(prior_neg) - (1./2.) * np.log(np.linalg.det(cov_neg))\n",
    "    \n",
    "    discri_pos = (-1/2)*np.dot(np.dot((X_test - mu_pos),inv_cov_pos), (X_test - mu_pos).T)\n",
    "    discri_pos = np.diag(discri_pos)\n",
    "    discri_pos = discri_pos + np.log(prior_pos) - (1./2.) * np.log(np.linalg.det(cov_pos))\n",
    "    \n",
    "    #classifiy to class -1 if discri_neg > discri_pos\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    Y_test_pred[discri_neg > discri_pos] = -1\n",
    "    Y_test_pred[discri_neg <= discri_pos] = +1\n",
    "    \n",
    "    return Y_test_pred\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell type : Convenience\n",
    "\n",
    "# # Testing the functions above\n",
    "\n",
    "# # To TAs: Replace this cell with the testing cell developed.\n",
    "\n",
    "# # To students: You may use the example here for testing syntax issues \n",
    "# # with your functions, and also as a sanity check. But the final evaluation\n",
    "# # will be done for different inputs to the functions. (So you can't just \n",
    "# # solve the problem for this one example given below.) \n",
    "\n",
    "\n",
    "X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "\n",
    "# Y_pred_test_1a = Bayes1a(X_train, Y_train, X_test)\n",
    "# Y_pred_test_1b = Bayes1b(X_train, Y_train, X_test)\n",
    "Y_pred_test_1c = Bayes1c(X_train, Y_train, X_test)\n",
    "\n",
    "\n",
    "# print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "\n",
    "# print('bayes1b error rate:', 1.0 -  (np.count_nonzero(Y_test == Y_pred_test_1b)/X_test.shape[0]))\n",
    "\n",
    "print('bayes1b error rate:', 1.0 -  (np.count_nonzero(Y_test == Y_pred_test_1c)/X_test.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "1d) Run the above three algorithms (Bayes1a,1b and 1c), for the three datasets given (dataset1_1.npz, dataset1_2.npz, dataset1_3.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 3 datasets = 9 plots) on a 2d plot (color the positively classified area light green, and negatively classified area light red). Add the training data points also on the plot. Plots to be organised into 3 plots follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 9 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise (use the plots of the data and the assumptions in the problem to explain) your observations regarding the six learnt classifiers, and also give the error rate of the three classifiers on the three datasets as 3x3 table, with appropriately named rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# fig = plt.subplots(nrows=3, ncols=3,figsize=(20,20))\n",
    "# datasets_1a = np.array(['dataset1_1', 'dataset1_2', 'dataset1_3'])\n",
    "# plots_pos = np.array([331,332,333])\n",
    "\n",
    "\n",
    "# for i in range(datasets_1a.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_1.npy')\n",
    "\n",
    "#     X_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_2.npy')\n",
    "#     Y_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_3.npy')\n",
    "\n",
    "#     gnb = GaussianNB()\n",
    "#     Y_pred_test_1a = gnb.fit(X_train, Y_train).predict(X_test)\n",
    "#     #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "#     error_rate = 1-0 - np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0]\n",
    "#     print('prior', gnb.class_prior_)\n",
    "#     print('mean', gnb.theta_)\n",
    "#     print('sigma', gnb.sigma_)\n",
    "    \n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes1a ' + datasets_1a[i] + ', error:' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == -1][:,0], X_test[Y_pred_test_1a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell type : CodeWrite\n",
    "# # write the code for loading the data, running the three algos, and plotting here. \n",
    "# # (Use the functions written previously.)\n",
    "# datasets_1a = np.array(['dataset1_1', 'dataset1_2', 'dataset1_3'])\n",
    "# fig2 = plt.subplots(ncols=3,figsize=(20,5))\n",
    "# plots_pos = np.array([131,132,133])\n",
    "# for i in range(datasets_1a.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_1.npy')\n",
    "\n",
    "#     plt.title(datasets_1a[i])\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     ax1.scatter(X_train[Y_train == -1][:,0], X_test[Y_train == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_train[Y_train == 1][:,0], X_test[Y_train == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')    \n",
    "    \n",
    "    \n",
    "\n",
    "# fig = plt.subplots(nrows=3, ncols=3,figsize=(20,20))\n",
    "# plots_pos = np.array([331,332,333])\n",
    "# for i in range(datasets_1a.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_1.npy')\n",
    "\n",
    "#     X_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_2.npy')\n",
    "#     Y_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_3.npy')\n",
    "\n",
    "#     Y_pred_test_1a = Bayes1a(X_train, Y_train, X_test)\n",
    "#     #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "#     error_rate = 1-0 - np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0]\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes1a ' + datasets_1a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == -1][:,0], X_test[Y_pred_test_1a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "    \n",
    "# # *********************plots for bayes 1b******************\n",
    "\n",
    "# plots_pos = np.array([334,335,336])\n",
    "\n",
    "# for i in range(datasets_1a.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_1.npy')\n",
    "\n",
    "#     X_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_2.npy')\n",
    "#     Y_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_3.npy')\n",
    "\n",
    "#     Y_pred_test_1a = Bayes1b(X_train, Y_train, X_test)\n",
    "#     #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "#     error_rate = 1-0 - np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0]\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes1c ' + datasets_1a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == -1][:,0], X_test[Y_pred_test_1a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # *********************plots for bayes1c******************\n",
    "\n",
    "# plots_pos = np.array([337,338,339])\n",
    "\n",
    "# for i in range(datasets_1a.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_1a[i] +'/arr_1.npy')\n",
    "\n",
    "#     X_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_2.npy')\n",
    "#     Y_test = np.load(file='Archive/'+ datasets_1a[i] +'/arr_3.npy')\n",
    "\n",
    "#     Y_pred_test_1a = Bayes1c(X_train, Y_train, X_test)\n",
    "#     #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "#     error_rate = 1-0 - np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0]\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes1c ' + datasets_1a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == -1][:,0], X_test[Y_pred_test_1a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Cell type : TextRead ** \n",
    "\n",
    "\n",
    "# Problem 2 : Learning Multiclass Bayes Classifiers from data with Max. Likeli.\n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. The $4\\times 4$ loss matrix giving the loss incurred for predicting $i$ when truth is $j$ is below.\n",
    "\n",
    "$L=\\begin{bmatrix} 0 &1 & 2& 3\\\\ 1 &0 & 1& 2\\\\ 2 &1 & 0& 1\\\\ 3 &2 & 1& 0 \\end{bmatrix}$ \n",
    "\n",
    "2a) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $I$.\n",
    "\n",
    "2b) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma$.\n",
    "\n",
    "2c) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma_a$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# Fill in functions in this cell\n",
    "\n",
    "\n",
    "def Bayes2a(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2a.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "#     print(X_train[0:50,:])\n",
    "#     print(Y_train[0:50])\n",
    "    X_1 = X_train[ Y_train == 1,:]\n",
    "    X_2 = X_train[ Y_train == 2,:]\n",
    "    X_3 = X_train[ Y_train == 3,:]\n",
    "    X_4 = X_train[ Y_train == 4,:]\n",
    "\n",
    "    \n",
    "    mu_1 = np.mean(X_1, axis=0)\n",
    "    mu_2 = np.mean(X_2, axis=0)\n",
    "    mu_3 = np.mean(X_3, axis=0)\n",
    "    mu_4 = np.mean(X_4, axis=0)\n",
    "    \n",
    "#     print('mu', mu_1, mu_2, mu_3, mu_4)\n",
    "    \n",
    "    prior_1 = X_1.shape[0]/X_train.shape[0]\n",
    "    prior_2 = X_2.shape[0]/X_train.shape[0]\n",
    "    prior_3 = X_3.shape[0]/X_train.shape[0]\n",
    "    prior_4 = X_4.shape[0]/X_train.shape[0]\n",
    "    \n",
    "#     print('prior', prior_1, prior_2, prior_3, prior_4)\n",
    "    \n",
    "    \n",
    "    #log posteriors\n",
    "#     post_Xtest_1 = (-1./2.)*np.sum((X_test - mu_1)**2,axis=1) + np.log(prior_1)\n",
    "#     post_Xtest_2 = (-1./2.)*np.sum((X_test - mu_2)**2,axis=1) + np.log(prior_2)\n",
    "#     post_Xtest_3 = (-1./2.)*np.sum((X_test - mu_3)**2,axis=1) + np.log(prior_3)\n",
    "#     post_Xtest_4 = (-1./2.)*np.sum((X_test - mu_4)**2,axis=1) + np.log(prior_4)\n",
    "    \n",
    "    \n",
    "    post_Xtest_1 = np.exp((-1./2.)*np.sum((X_test - mu_1)**2,axis=1))*prior_1\n",
    "    post_Xtest_2 = np.exp((-1./2.)*np.sum((X_test - mu_2)**2,axis=1))*prior_2\n",
    "    post_Xtest_3 = np.exp((-1./2.)*np.sum((X_test - mu_3)**2,axis=1))*prior_3\n",
    "    post_Xtest_4 = np.exp((-1./2.)*np.sum((X_test - mu_4)**2,axis=1))*prior_4\n",
    "\n",
    "#     post_Xtest_1 = (1/np.power(2*np.pi, X_test.shape[i]/2))*np.exp((-1./2.)*np.sum((X_test - mu_1)**2, axis=1))*np.log(prior_1)\n",
    "#     post_Xtest_2 = (1/np.power(2*np.pi, X_test.shape[i]/2))*np.exp((-1./2.)*np.sum((X_test - mu_2)**2, axis=1))*np.log(prior_2)\n",
    "#     post_Xtest_3 = (1/np.power(2*np.pi, X_test.shape[i]/2))*np.exp((-1./2.)*np.sum((X_test - mu_3)**2, axis=1))*np.log(prior_3)\n",
    "#     post_Xtest_4 = (1/np.power(2*np.pi, X_test.shape[i]/2))*np.exp((-1./2.)*np.sum((X_test - mu_4)**2, axis=1))*np.log(prior_4)\n",
    "    \n",
    "#     print(post_Xtest_1.shape)\n",
    "\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    loss_matrix = np.array([[0,1,2,3],\n",
    "                            [1,0,1,2],\n",
    "                            [2,1,0,1],\n",
    "                            [3,2,1,0]])\n",
    "    \n",
    "#     loss_matrix = np.ones(shape=(4,4)) - np.eye(4)\n",
    "    \n",
    "#     print('loss', loss_matrix)\n",
    "\n",
    "    #action matrix is k x N , wherer k-classes and n-test points\n",
    "    post_matrix = np.array([post_Xtest_1, post_Xtest_2, post_Xtest_3, post_Xtest_4])\n",
    "#     print(R_Xtest_1.shape)\n",
    "#     print(post_matrix.shape)\n",
    "    \n",
    "    Y_test_pred = np.argmin(np.dot(loss_matrix, post_matrix), axis=0) + 1\n",
    "    \n",
    "#     print('post_matrix', post_matrix[:,0:5])\n",
    "#     print('Ytest',Y_test_pred[0:5])\n",
    "#     print(X_test.shape)\n",
    "#     print(g_Xtest.shape)\n",
    "    \n",
    "    return Y_test_pred\n",
    "    \n",
    "    \n",
    "def Bayes2b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_1 = X_train[ Y_train == 1,:]\n",
    "    X_2 = X_train[ Y_train == 2,:]\n",
    "    X_3 = X_train[ Y_train == 3,:]\n",
    "    X_4 = X_train[ Y_train == 4,:]\n",
    "\n",
    "    \n",
    "    mu_1 = np.mean(X_1, axis=0)\n",
    "    mu_2 = np.mean(X_2, axis=0)\n",
    "    mu_3 = np.mean(X_3, axis=0)\n",
    "    mu_4 = np.mean(X_4, axis=0)\n",
    "    \n",
    "    \n",
    "    prior_1 = X_1.shape[0]/X_train.shape[0]\n",
    "    prior_2 = X_2.shape[0]/X_train.shape[0]\n",
    "    prior_3 = X_3.shape[0]/X_train.shape[0]\n",
    "    prior_4 = X_4.shape[0]/X_train.shape[0]\n",
    "    \n",
    "    \n",
    "    comm_cov = np.cov(X_train.T)\n",
    "    \n",
    "    inv_cov = np.linalg.inv(comm_cov)\n",
    "    \n",
    "    #log posteriors\n",
    "    post_Xtest_1 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_1),inv_cov), (X_test - mu_1).T)))*(prior_1)\n",
    "    post_Xtest_2 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_2),inv_cov), (X_test - mu_2).T)))*(prior_2)\n",
    "    post_Xtest_3 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_3),inv_cov), (X_test - mu_3).T)))*(prior_3)\n",
    "    post_Xtest_4 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_4),inv_cov), (X_test - mu_4).T)))*(prior_4)\n",
    "\n",
    "\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    loss_matrix = np.array([[0,1,2,3],\n",
    "                            [1,0,1,2],\n",
    "                            [2,1,0,1],\n",
    "                            [3,2,1,0]])\n",
    "    \n",
    "    #action matrix is k x N , wherer k-classes and n-test points\n",
    "    post_matrix = np.array([post_Xtest_1, post_Xtest_2, post_Xtest_3, post_Xtest_4])\n",
    "    \n",
    "    Y_test_pred = np.argmin(np.dot(loss_matrix, post_matrix), axis=0) + 1\n",
    "    \n",
    "    return Y_test_pred\n",
    "    \n",
    "    \n",
    "\n",
    "def Bayes2c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_1 = X_train[ Y_train == 1,:]\n",
    "    X_2 = X_train[ Y_train == 2,:]\n",
    "    X_3 = X_train[ Y_train == 3,:]\n",
    "    X_4 = X_train[ Y_train == 4,:]\n",
    "\n",
    "    \n",
    "    mu_1 = np.mean(X_1, axis=0)\n",
    "    mu_2 = np.mean(X_2, axis=0)\n",
    "    mu_3 = np.mean(X_3, axis=0)\n",
    "    mu_4 = np.mean(X_4, axis=0)\n",
    "    \n",
    "    \n",
    "    prior_1 = X_1.shape[0]/X_train.shape[0]\n",
    "    prior_2 = X_2.shape[0]/X_train.shape[0]\n",
    "    prior_3 = X_3.shape[0]/X_train.shape[0]\n",
    "    prior_4 = X_4.shape[0]/X_train.shape[0]\n",
    "    \n",
    "    \n",
    "    cov_1 = np.cov(X_1.T)\n",
    "    cov_2 = np.cov(X_2.T)\n",
    "    cov_3 = np.cov(X_3.T)\n",
    "    cov_4 = np.cov(X_4.T)\n",
    "    \n",
    "    inv_cov_1 = np.linalg.inv(cov_1)\n",
    "    inv_cov_2 = np.linalg.inv(cov_2)\n",
    "    inv_cov_3 = np.linalg.inv(cov_3)\n",
    "    inv_cov_4 = np.linalg.inv(cov_4)\n",
    "    \n",
    "#     discri_neg = (-1/2)*np.dot(np.dot((X_test - mu_neg),inv_cov_neg), (X_test - mu_neg).T)\n",
    "#     discri_neg = np.diag(discri_neg)\n",
    "#     discri_neg = discri_neg + np.log(prior_neg) - (1./2.) * np.log(np.linalg.det(cov_neg))\n",
    "    \n",
    "#     discri_pos = (-1/2)*np.dot(np.dot((X_test - mu_pos),inv_cov_pos), (X_test - mu_pos).T)\n",
    "#     discri_pos = np.diag(discri_pos)\n",
    "#     discri_pos = discri_pos + np.log(prior_pos) - (1./2.) * np.log(np.linalg.det(cov_pos))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #log posteriors\n",
    "#     post_Xtest_1 = np.diag((-1./2.)*np.dot(np.dot((X_test - mu_1),inv_cov_1), (X_test - mu_1).T)) + np.log(prior_1) - (1./2.) * np.log(np.linalg.det(cov_1))\n",
    "#     post_Xtest_2 = np.diag((-1./2.)*np.dot(np.dot((X_test - mu_2),inv_cov_2), (X_test - mu_2).T)) + np.log(prior_2) - (1./2.) * np.log(np.linalg.det(cov_2))\n",
    "#     post_Xtest_3 = np.diag((-1./2.)*np.dot(np.dot((X_test - mu_3),inv_cov_3), (X_test - mu_3).T)) + np.log(prior_3) - (1./2.) * np.log(np.linalg.det(cov_3))\n",
    "#     post_Xtest_4 = np.diag((-1./2.)*np.dot(np.dot((X_test - mu_4),inv_cov_4), (X_test - mu_4).T)) + np.log(prior_4) - (1./2.) * np.log(np.linalg.det(cov_4))\n",
    "\n",
    "    post_Xtest_1 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_1),inv_cov_1), (X_test - mu_1).T)))*(prior_1)*(1./np.sqrt(np.linalg.det(cov_1)))\n",
    "    post_Xtest_2 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_2),inv_cov_2), (X_test - mu_2).T)))*(prior_2)*(1./np.sqrt(np.linalg.det(cov_2)))\n",
    "    post_Xtest_3 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_3),inv_cov_3), (X_test - mu_3).T)))*(prior_3)*(1./np.sqrt(np.linalg.det(cov_3)))\n",
    "    post_Xtest_4 = np.exp(np.diag((-1./2.)*np.dot(np.dot((X_test - mu_4),inv_cov_4), (X_test - mu_4).T)))*(prior_4)*(1./np.sqrt(np.linalg.det(cov_4)))\n",
    "\n",
    "    \n",
    "    \n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    loss_matrix = np.array([[0,1,2,3],\n",
    "                            [1,0,1,2],\n",
    "                            [2,1,0,1],\n",
    "                            [3,2,1,0]])\n",
    "    \n",
    "#     loss_matrix = np.ones(shape=(4,4)) - np.eye(4)\n",
    "\n",
    "    #action matrix is k x N , wherer k-classes and n-test points\n",
    "    post_matrix = np.array([post_Xtest_1, post_Xtest_2, post_Xtest_3, post_Xtest_4])\n",
    "    \n",
    "    Y_test_pred = np.argmin(np.dot(loss_matrix, post_matrix), axis=0) + 1\n",
    "    \n",
    "    return Y_test_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : Convenience\n",
    "\n",
    "# Testing the functions above\n",
    "\n",
    "# Data 1\n",
    "\n",
    "mat1=np.array([[1.,0.],[0.,1.]])\n",
    "mat2=np.array([[1.,0.],[0.,1.]])\n",
    "mat3=np.array([[1.,0.],[0.,1.]])\n",
    "mat4=np.array([[1.,0.],[0.,1.]])\n",
    "\n",
    "X_train_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "X_train_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "X_train_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "X_train_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "\n",
    "X_train = np.concatenate((X_train_1, X_train_2, X_train_3, X_train_4), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "\n",
    "\n",
    "X_test_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "X_test_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "X_test_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "X_test_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "\n",
    "X_test = np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4), axis=0)\n",
    "Y_test = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "\n",
    "\n",
    "\n",
    "# Y_pred_test_2a = Bayes2a(X_train, Y_train, X_test)\n",
    "Y_pred_test_2b = Bayes2b(X_train, Y_train, X_test)\n",
    "# Y_pred_test_2c = Bayes2c(X_train, Y_train, X_test)\n",
    "\n",
    "# print(Y_pred_test_2a)\n",
    "# print('bayes2a error rate:', 1.0 -  (np.count_nonzero(Y_test == Y_pred_test_2a)/X_test.shape[0]))\n",
    "print('bayes2b error rate:', 1.0 -  (np.count_nonzero(Y_test == Y_pred_test_2b)/X_test.shape[0]))\n",
    "# print('bayes2c error rate:', 1.0 -  (np.count_nonzero(Y_test == Y_pred_test_2c)/X_test.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 2\n",
    "\n",
    "2d) Run the above three algorithms (Bayes2a,2b and 2c), for the two datasets given (dataset2_1.npz, dataset2_2.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 2 datasets = 6 plots) on a 2d plot (color the 4 areas classified as 1,2,3 and 4 differently). Add the training data points also on the plot. Plots to be organised as follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 6 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise your observations regarding the six learnt classifiers. Give the *expected loss* (use the Loss matrix given in the problem.) of the three classifiers on the two datasets as 2x3 table, with appropriately named rows and columns. Also, give the 4x4 confusion matrix of the final classifier for all three algorithms and both datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "# Cell type : CodeWrite\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "\n",
    "datasets_2 = np.array(['dataset2_1', 'dataset2_2'])\n",
    "\n",
    "# fig = plt.subplots(nrows=3, ncols=2,figsize=(20,20))\n",
    "plots_pos = np.array([321,322,323])\n",
    "for i in range(datasets_2.shape[0]):\n",
    "    X_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_0.npy')\n",
    "    Y_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_1.npy')\n",
    "\n",
    "    X_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_2.npy')\n",
    "    Y_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_3.npy')\n",
    "\n",
    "    Y_pred_test_2a = Bayes2a(X_train, Y_train, X_test)\n",
    "    #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "    error_rate = 1.0 - np.count_nonzero(Y_test == Y_pred_test_2a)/X_test.shape[0]\n",
    "    print('bayes2a', error_rate)\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes2a ' + datasets_2a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_2a == -1][:,0], X_test[Y_pred_test_2a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "\n",
    "for i in range(datasets_2.shape[0]):\n",
    "    X_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_0.npy')\n",
    "    Y_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_1.npy')\n",
    "\n",
    "    X_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_2.npy')\n",
    "    Y_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_3.npy')\n",
    "\n",
    "    Y_pred_test_2b = Bayes2b(X_train, Y_train, X_test)\n",
    "    #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "    error_rate = 1.0 - np.count_nonzero(Y_test == Y_pred_test_2b)/X_test.shape[0]\n",
    "    print('bayes2b', error_rate)\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes2a ' + datasets_2a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_2a == -1][:,0], X_test[Y_pred_test_2a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "\n",
    "for i in range(datasets_2.shape[0]):\n",
    "    X_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_0.npy')\n",
    "    Y_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_1.npy')\n",
    "\n",
    "    X_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_2.npy')\n",
    "    Y_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_3.npy')\n",
    "\n",
    "    Y_pred_test_2c = Bayes2c(X_train, Y_train, X_test)\n",
    "    #print('bayes1a test accuracy:', np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0])\n",
    "    \n",
    "    error_rate = 1.0 - np.count_nonzero(Y_test == Y_pred_test_2c)/X_test.shape[0]\n",
    "    print('bayes2c', error_rate)\n",
    "\n",
    "#     ax1 = plt.subplot(plots_pos[i])\n",
    "#     plt.title('bayes2a ' + datasets_2a[i] +', error=' + str(error_rate))\n",
    "\n",
    "#     ax1.scatter(X_test[Y_pred_test_2a == -1][:,0], X_test[Y_pred_test_2a == -1][:,1],\n",
    "#                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "#     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "#                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "#     ax1.legend(loc='upper left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# datasets_2 = np.array(['dataset2_1', 'dataset2_2'])\n",
    "\n",
    "# # fig = plt.subplots(nrows=3, ncols=2,figsize=(20,20))\n",
    "# plots_pos = np.array([321,322,323])\n",
    "\n",
    "# for i in range(datasets_2.shape[0]):\n",
    "#     X_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_0.npy')\n",
    "#     Y_train = np.load(file='Archive/'+ datasets_2[i] +'/arr_1.npy')\n",
    "\n",
    "#     X_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_2.npy')\n",
    "#     Y_test = np.load(file='Archive/'+ datasets_2[i] +'/arr_3.npy')\n",
    "\n",
    "#     gnb = GaussianNB()\n",
    "#     Y_pred_test_2a = gnb.fit(X_train, Y_train).predict(X_test)\n",
    "#     print('bayes2 error rate:', 1.0 - np.count_nonzero(Y_test == Y_pred_test_2a)/X_test.shape[0])\n",
    "    \n",
    "# #     error_rate = 1-0 - np.count_nonzero(Y_test == Y_pred_test_1a)/X_test.shape[0]\n",
    "# #     print('prior', gnb.class_prior_)\n",
    "# #     print('mean', gnb.theta_)\n",
    "# #     print('sigma', gnb.sigma_)\n",
    "    \n",
    "# #     ax1 = plt.subplot(plots_pos[i])\n",
    "# #     plt.title('bayes1a ' + datasets_1a[i] + ', error:' + str(error_rate))\n",
    "\n",
    "# #     ax1.scatter(X_test[Y_pred_test_1a == -1][:,0], X_test[Y_pred_test_1a == -1][:,1],\n",
    "# #                     alpha=0.2, color='r', marker='o', label='neg')\n",
    "# #     ax1.scatter(X_test[Y_pred_test_1a == 1][:,0], X_test[Y_pred_test_1a == 1][:,1], \n",
    "# #                     alpha=0.2, color='g', marker='o', label='pos')\n",
    "# #     ax1.legend(loc='upper left')\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead **\n",
    "\n",
    "# Problem 3 : Bias-Variance analysis in regression\n",
    "\n",
    "Do bias variance analysis for the following setting: \n",
    "\n",
    "$X \\sim Unif([-1,1]\\times[-1,1])$\n",
    "\n",
    "$Y=\\exp(-4*||X-a||^2) + \\exp(-4*||X-b||^2) + \\exp(-4*||X-c||^2)$\n",
    "\n",
    "where $a=[0.5,0.5], b=[-0.5,0.5], c=[0.5, -0.5]$.\n",
    "\n",
    "Regularised Risk = $\\frac{1}{m} \\sum_{i=1}^m (w^\\top \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} ||w||^2 $ \n",
    "\n",
    "Sample 50 (X,Y) points from above distribution, and do ridge regularised polynomial regression with degrees=[1,2,4,8,16] and regularisation parameters ($\\lambda$) = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]. Repeat for 100 times, and estimate the bias and variance for all 15 algorithms. You may approximate the distribution over X by discretising the $[-1,1]\\times[-1,1]$ space into 10000 points. (Both expectations over S and (x,y) are simply estimates due to the finiteness of our experiments and sample)\n",
    " \n",
    "3a) For each of the 30 algorithms (corresponding to 5 degrees and 6 lambda values) analyse the contour plot of the estimated $f_S$ for 3 different training sets. And the average $g(x) = E_S [f_S(x)]$. Write one function for doing everything in the code cell below. \n",
    "\n",
    "3b) In the next text cell, give the Bias and Variance computed as a $5\\times 6$ matrix, appropriately label the rows and columns. And give your conclusion in one or two sentences. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea of Ridge Regression\n",
    "***\n",
    "- If weights become too large, it's overfits the data (low bias but hight variance).\n",
    "- to solve this, we can assume prior $Pr(w) \\sim Normal(0, I)$\n",
    "- **MAP** estimate of **w** is \n",
    "  $\\hat{\\mathbf{w}} = (λI_D + X^T X)^{−1}X^T y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates all term of so that sum of degree of components of x is equal to deg\n",
    "#example deg = 2, x = [x1, x2] then x1^0 * x2^2, x1^1 * x2^1, x1^2 * x2^0\n",
    "def phi_givendegree(deg_to_exand, x):\n",
    "    #this will create empty array of shape (x.shape , 0)\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    #d the dimension of data x\n",
    "    d = x.shape[1]\n",
    "    all_terms = np.empty(shape=(n, 0))\n",
    "    for i in range(deg_to_exand+1):\n",
    "        all_terms = np.concatenate((all_terms, (x[:,0][:,None]**(np.abs(d-i) )* (x[:,1][:,None]**i))), axis=1)\n",
    "        \n",
    "    return all_terms\n",
    "        \n",
    "\n",
    "# # test phi_givendegree\n",
    "# a = np.array([[1,2],[4,8],[4,7]])\n",
    "# b = np.empty(shape=(3,0))\n",
    "\n",
    "# np.append(a,a[:, 1][:,None], axis=1)\n",
    "# b.shape\n",
    "\n",
    "# a[:, 0][:,None]**2\n",
    "\n",
    "# np.concatenate((b, a[:,1][:,None]), axis=1)\n",
    "\n",
    "# a = np.array([[1,2],[1,2],[1,2]])\n",
    "# phi_givendegree(2, a)\n",
    "# # x = a\n",
    "# # n = x.shape[0]\n",
    "# # d = x.shape[1]\n",
    "# # all_terms = np.empty(shape=(n, 0))\n",
    "# # (x[:,0][:,None]**(np.abs(d-i) )* (x[:,1][:,None]**i))\n",
    "\n",
    "# # np.\n",
    "\n",
    "# # x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_expansion_fun(deg, x):\n",
    "    n = x.shape[0]\n",
    "    phi = np.empty(shape=(n,0))\n",
    "    \n",
    "    for i in range(deg+1):\n",
    "        phi = np.concatenate((phi, phi_givendegree(deg_to_exand=i, x=x)), axis=1)\n",
    "        #print('i=',i, '\\n',phi)\n",
    "    \n",
    "    return phi\n",
    "\n",
    "\n",
    "# # test code\n",
    "# print(a)\n",
    "# basis_expansion_fun(3,a)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(sample_size=50):\n",
    "    feature1 = np.random.uniform(low=-1.0, high=1.0, size=sample_size).reshape(sample_size,1)\n",
    "    feature2 = np.random.uniform(low=-1.0, high=1.0, size=sample_size).reshape(sample_size,1)\n",
    "\n",
    "    # print(feature1)\n",
    "    # print(feature2)\n",
    "\n",
    "    x = np.concatenate((feature1, feature2), axis=1)\n",
    "    y = compute_y(x)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "def compute_y(x):\n",
    "    a = np.array([0.5,0.5])\n",
    "    b = np.array([-0.5,0.5])\n",
    "    c = np.array([0.5,-0.5])\n",
    "    y = np.exp( -4*np.sum((x-a)**2, axis=1)) + np.exp( -4*np.sum((x-b)**2, axis=1)) + np.exp( -4*np.sum((x-c)**2, axis=1))\n",
    "    \n",
    "    return y\n",
    "\n",
    "# print('samples', samples(sample_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1, X2 = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "# # print(X1)\n",
    "# # print(X2) \n",
    "\n",
    "# Z = np.zeros(X1.shape)\n",
    "\n",
    "# for i in range(Z.shape[0]):\n",
    "#     for j in range(Z.shape[1]):\n",
    "#         point = np.array([[X1[i,j], X2[i,j]]])\n",
    "#         Z[i,j] = compute_y(point)\n",
    "\n",
    "# plt.contourf(X1,X2, Z, levels=np.linspace(0.,1.2 , 20))  \n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_param = 0.\n",
    "# degree = 8\n",
    "\n",
    "# compute weight vector\n",
    "def polynomial_regression_ridge_train(X_train, Y_train, degree=1, reg_param=0.01):\n",
    "    \"\"\" Give best polynomial fitting data, based on empirical squared error minimisation.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: numpy array of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "    w : numpy array of shape (d',) with appropriate d'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    design_matrix = basis_expansion_fun(deg=degree, x=X_train)\n",
    "    \n",
    "    weight = reg_param*np.identity(n= design_matrix.shape[1]) + np.dot(design_matrix.T, design_matrix)\n",
    "    weight = np.linalg.pinv(weight)\n",
    "    weight = np.dot(weight, np.dot(design_matrix.T, Y_train))\n",
    "\n",
    "    return weight   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# weig = polynomial_regression_ridge_train(X_train=x, Y_train=y, degree=degree, reg_param=reg_param)\n",
    "# print(weig)\n",
    "# print(weig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_ridge_pred(X_test, wt_vector, degree=1):\n",
    "    \"\"\" Give the value of the learned polynomial function, on test data.\n",
    "\n",
    "    Arguments:\n",
    "    X_test: numpy array of shape (n,d)\n",
    "    wt_vec: numpy array of shape (d',)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : numpy array of shape (n,)\n",
    "    \n",
    "    \"\"\"\n",
    "    design_matrix = basis_expansion_fun(deg=degree, x=X_test)\n",
    "    Y_predicted = np.dot(design_matrix, wt_vector)\n",
    "    return Y_predicted.ravel()\n",
    "\n",
    "# y_predicted = polynomial_regression_ridge_pred(X_test=x, wt_vector=weig, degree=degree)\n",
    "\n",
    "# print('actual y', y)\n",
    "# print('predicted y', y_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def visualise_polynomial_2d(wt_vector, degree, title=\"\"):\n",
    "\n",
    "#     X,Y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "    \n",
    "#     Z = np.zeros(X.shape)\n",
    "    \n",
    "#     for i in range(Z.shape[0]):\n",
    "#         for j in range(Z.shape[1]):\n",
    "#             point = np.array([[X[i,j], Y[i,j]]])\n",
    "#             Z[i,j] = polynomial_regression_ridge_pred(X_test=point, wt_vector=weig, degree=degree)\n",
    "\n",
    "#     plt.contourf(X,Y,Z,levels=np.linspace(0.,1.2 , 20))\n",
    "#     plt.title('learned function : degree= '+ str(degree)+ ' , reg_param=' + title)\n",
    "#     plt.colorbar()\n",
    "    \n",
    "\n",
    "# visualise_polynomial_2d(wt_vector=weig, degree=degree, title=str(reg_param))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw originalfunc and predicted func contours\n",
    "\n",
    "# def my_visualise_polynomial_2d(wt_vector, degree, title=\"\"):\n",
    "\n",
    "#     X1, X2 = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "#     # print(X1)\n",
    "#     # print(X2) \n",
    "\n",
    "#     Z_orig = np.zeros(X1.shape)\n",
    "#     Z_pred = np.zeros(X1.shape)\n",
    "\n",
    "#     for i in range(Z.shape[0]):\n",
    "#         for j in range(Z.shape[1]):\n",
    "#             point = np.array([[X1[i,j], X2[i,j]]])\n",
    "#             Z_orig[i,j] = compute_y(point)\n",
    "#             Z_pred[i,j] = polynomial_regression_ridge_pred(X_test=point, wt_vector=weig, degree=degree)\n",
    "\n",
    "#     fig,(ax1, ax2) = plt.subplots(nrows=2, figsize=(8,8))\n",
    "    \n",
    "#     ax1.axis((-1,1,-1,1))\n",
    "#     cntr1 = ax1.contourf(X1, X2, Z_orig,levels=np.linspace(0.,1.2 , 20) ) \n",
    "#     ax1.set_title('original')\n",
    "#     fig.colorbar(cntr1, ax= ax1)\n",
    "    \n",
    "#     ax2.axis((-1,1,-1,1))\n",
    "#     cntr2 = ax2.contourf(X1, X2, Z_pred,levels=np.linspace(0.,1.2 , 20))\n",
    "#     ax2.set_title('learned function : degree= '+ str(degree)+ ' , reg_param=' + title)\n",
    "#     fig.colorbar(cntr2, ax=ax2)\n",
    "    \n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# my_visualise_polynomial_2d(wt_vector=weig, degree=degree, title=str(reg_param))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_BV_error_sample_plot(degree, reg_param, num_training_samples=50):\n",
    "    \n",
    "    \"\"\"Write code for generating data, fitting polynomial for given degree and reg_param. \n",
    "    Use num_training_samples samples for training.\n",
    "        \n",
    "    Compute the $f_S$ of 100 runs. \n",
    "\n",
    "    Plot 3 examples of learned function to illustrate how learned function varies \n",
    "    with different training samples. Also plot the average $f_S$ of all 100 runs.\n",
    "    \n",
    "    In total 4 subplots in one plot with appropriate title including degree and lambda value.\n",
    "    \n",
    "    Fill code to compute bias and variance, and average mean square error using the computed 100 $f_S$ functions.\n",
    "    \n",
    "    All contourplots are to be drawn with levels=np.linspace(0,1.2,20)\n",
    "    \n",
    "    Also return bias, variance, mean squared error. \"\"\"\n",
    "    \n",
    "    #find 100 weights for diff num_training_samples\n",
    "    #d' = (k+d)C_d \n",
    "    d_new = int(((degree+2)*(degree + 1))/2)\n",
    "#     print(d_new)\n",
    "    weights = np.empty(shape=(100,d_new))\n",
    "    for i in range(100):\n",
    "        x, y = samples(num_training_samples)\n",
    "        \n",
    "        weights[i] = np.array([polynomial_regression_ridge_train(x, y, degree=degree, reg_param=reg_param)])\n",
    "    \n",
    "    #estimate f_s over grid\n",
    "    X1, X2 = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "#     Z_estimated_f = np.zeros(X1.shape)\n",
    "#     for i in range(Z_estimated_f.shape[0]):\n",
    "#         for j in range(Z_estimated_f.shape[1]):\n",
    "#             point = np.array([[X1[i,j], X2[i,j]]])\n",
    "            \n",
    "#             for r in range(100):\n",
    "#                 Z_estimated_f[i,j] += polynomial_regression_ridge_pred(X_test=point, wt_vector=weights[r], degree=degree)\n",
    "            \n",
    "#             Z_estimated_f[i,j] /= 100\n",
    "    \n",
    "    avg_weight = np.mean(weights, axis=0)\n",
    "    \n",
    "    MSE_error = 0.0\n",
    "    bias_square_error = 0.0\n",
    "    variance_error = 0.0\n",
    "    \n",
    "    for i in range(X1.shape[0]):\n",
    "        for j in range(X1.shape[1]):\n",
    "            point = np.array([[X1[i,j], X2[i,j]]])\n",
    "            true_value = compute_y(point)\n",
    "            avg_value = polynomial_regression_ridge_pred(point, avg_weight,degree)\n",
    "            \n",
    "            bias_square_error += (avg_value - true_value)**2\n",
    "            \n",
    "            for r in range(100):\n",
    "                predicted_value = polynomial_regression_ridge_pred(point,weights[r],degree)\n",
    "                MSE_error += (predicted_value - true_value)**2\n",
    "                variance_error += (avg_value - predicted_value)**2\n",
    "            \n",
    "            MSE_error /= 100\n",
    "            variance_error /= 100\n",
    "    \n",
    "    print('bias error', bias_square_error**(1/2), 'variance err', variance_error , 'MSE error', MSE_error)\n",
    "    \n",
    "    \n",
    "\n",
    "# compute_BV_error_sample_plot(degree=2, reg_param=0.0, num_training_samples=50)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias error 0.1975559637888873 variance err 0.039113453518722176 MSE error 0.23666941730760946\n"
     ]
    }
   ],
   "source": [
    "def compute_BV_error_sample_plot_v2(degree, reg_param, num_training_samples=50):\n",
    "    \n",
    "    \"\"\"Write code for generating data, fitting polynomial for given degree and reg_param. \n",
    "    Use num_training_samples samples for training.\n",
    "        \n",
    "    Compute the $f_S$ of 100 runs. \n",
    "\n",
    "    Plot 3 examples of learned function to illustrate how learned function varies \n",
    "    with different training samples. Also plot the average $f_S$ of all 100 runs.\n",
    "    \n",
    "    In total 4 subplots in one plot with appropriate title including degree and lambda value.\n",
    "    \n",
    "    Fill code to compute bias and variance, and average mean square error using the computed 100 $f_S$ functions.\n",
    "    \n",
    "    All contourplots are to be drawn with levels=np.linspace(0,1.2,20)\n",
    "    \n",
    "    Also return bias, variance, mean squared error. \"\"\"\n",
    "    \n",
    "    num_of_runs = 100\n",
    "    #find 100 weights for diff num_training_samples\n",
    "    #d' = (k+d)C_d \n",
    "    d_new = int(((degree+2)*(degree + 1))/2)\n",
    "    weights = np.empty(shape=(num_of_runs,d_new))\n",
    "    for i in range(num_of_runs):\n",
    "        x, y = samples(num_training_samples)\n",
    "        weights[i] = np.array([polynomial_regression_ridge_train(x, y, degree=degree, reg_param=reg_param)])\n",
    "        \n",
    "#     print('weights vec', weights)\n",
    "    \n",
    "\n",
    "    \n",
    "    x1_x2points = np.array(np.meshgrid(np.linspace(-1,1,1), np.linspace(-1,1,1))).T.reshape(-1,2)\n",
    "#     print('points', x1_x2points)\n",
    "    \n",
    "    MSE_error = 0.0\n",
    "    bias_square_error = 0.0\n",
    "    variance_error = 0.0\n",
    "    \n",
    "    variance_error_ary = np.zeros(shape=(x1_x2points.shape[0],))\n",
    "    mse_error_ary = np.zeros(shape=(x1_x2points.shape[0],))\n",
    "    \n",
    "    \n",
    "    true_func = compute_y(x1_x2points)\n",
    "#     print('ture fun', true_func)\n",
    "    \n",
    "    avg_weight = np.mean(weights, axis=0)\n",
    "#     print('avg wei', avg_weight)\n",
    "    \n",
    "    avg_func = polynomial_regression_ridge_pred(X_test=x1_x2points, wt_vector=avg_weight, degree=degree)\n",
    "#     print('avg_fun', avg_func)\n",
    "    \n",
    "    bias_square_error = np.sum((1/avg_func.shape[0])*((avg_func - true_func)**2),axis=0)\n",
    "#     print('bs err', bias_square_error)\n",
    "    \n",
    "    #compute variance error and mse in same loop\n",
    "    for i in range(num_of_runs):\n",
    "        func_sample_i = polynomial_regression_ridge_pred(x1_x2points, weights[i],degree)\n",
    "        mse_error_ary += (func_sample_i - true_func)**2\n",
    "        variance_error_ary +=  (func_sample_i - avg_func)**2\n",
    "    \n",
    "    variance_error = (1/variance_error_ary.shape[0])*(np.sum((1/num_of_runs)*variance_error_ary, axis=0))\n",
    "    MSE_error = np.sum((1/num_of_runs)*mse_error_ary,axis=0)\n",
    "    \n",
    "    \n",
    "    print('bias error', bias_square_error, 'variance err', variance_error , 'MSE error', MSE_error)\n",
    "    \n",
    "    \n",
    "\n",
    "compute_BV_error_sample_plot_v2(degree=1, reg_param=0.0, num_training_samples=50)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)\n",
    "t[0,0] + t[0,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "\n",
    "def polynomial_regression_ridge_pred(X_test, wt_vector, degree=1):\n",
    "    \"\"\" Give the value of the learned polynomial function, on test data.\n",
    "\n",
    "    Arguments:\n",
    "    X_test: numpy array of shape (n,d)\n",
    "    wt_vec: numpy array of shape (d',)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : numpy array of shape (n,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def visualise_polynomial_2d(wt_vector, degree, title=\"\"):\n",
    "\n",
    "    X,Y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "\n",
    "    plt.contourf(X,Y,Z,levels=np.linspace(0.,1.2 , 20))\n",
    "    plt.title('learned function : degree= '+ str(degree) + title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def polynomial_regression_ridge_train(X_train, Y_train, degree=1, reg_param=0.01):\n",
    "    \"\"\" Give best polynomial fitting data, based on empirical squared error minimisation.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: numpy array of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "    w : numpy array of shape (d',) with appropriate d'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def compute_BV_error_sample_plot(degree, reg_param, num_training_samples=50):\n",
    "    \n",
    "    \"\"\"Write code for generating data, fitting polynomial for given degree and reg_param. \n",
    "    Use num_training_samples samples for training.\n",
    "        \n",
    "    Compute the $f_S$ of 100 runs. \n",
    "\n",
    "    Plot 3 examples of learned function to illustrate how learned function varies \n",
    "    with different training samples. Also plot the average $f_S$ of all 100 runs.\n",
    "    \n",
    "    In total 4 subplots in one plot with appropriate title including degree and lambda value.\n",
    "    \n",
    "    Fill code to compute bias and variance, and average mean square error using the computed 100 $f_S$ functions.\n",
    "    \n",
    "    All contourplots are to be drawn with levels=np.linspace(0,1.2,20)\n",
    "    \n",
    "    Also return bias, variance, mean squared error. \"\"\"\n",
    "    \n",
    " \n",
    "    \n",
    "for degree in [1,2,4]:\n",
    "    for reg_param in [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]:\n",
    "        plt.figure()\n",
    "        b,v,e = compute_BV_error_sample_plot_v2(degree, reg_param)\n",
    "        print('================================')\n",
    "        print('Degree= '+str(degree)+' lambda= '+str(reg_param))\n",
    "        print('Bias = '+str(b))\n",
    "        print('Variance = '+str(v))\n",
    "        print('MSE = '+str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type: convenience\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type: TextWrite **\n",
    "Give the biases and variances computed for the various algorithms with various degrees and lambdas and summarise your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextRead **\n",
    "\n",
    "# Problem 4 : Analyse overfitting and underfitting in Regression\n",
    "\n",
    "\n",
    "Consider the 2-dimensional regression dataset \"dateset4_1.npz\". Do polynomial ridge regression for degrees = [1,2,4,8,16], and regularisation parameter $\\lambda$ = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]. Do all the above by using three different subset sizes of the training set : 50, 100, 200 and 1000. (Just take the first few samples of X_train and Y_train.)\n",
    "\n",
    "Regularised Risk = $\\frac{1}{m} \\sum_{i=1}^m (w^\\top \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} ||w||^2 $ \n",
    "\n",
    "The lambda value is given by the regularisation parameter.\n",
    "\n",
    "In the next codewrite cell, for each training set size compute how the train and test squared error varies with degree and regularisation parameter. Compute the \"best\" degree and regularisation parameter based on the test squared error. Give a contour plot of the learned function for the chosen hyper-parameters, with appropriate title including the hyperparameters. Total number of figures = 4 (one for each training set size.)\n",
    "\n",
    "Summarise your findings in the next tex cell in a few sentences. And reproduce the tables showing train and test error for various training sizes, with appropriate row and column names.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
